# -*- coding: utf-8 -*-
"""This_Toon_Does_Not_Exist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10xoEE52PitaBM8uqc6nnbal2oujJkuff

### Importing Modules
"""

#comment

pip install d2l # Note: If prompted to restart, please do so and run the proceeding cell.

import torch
import torchvision
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import warnings
import zipfile
from tqdm import tqdm
import os
warnings.filterwarnings('ignore')

from torch import nn
from d2l import torch as d2l

"""### Creating the Toon Dataset"""

# Downloading dataset
!kaggle datasets download -d brendanartley/cartoon-faces-googles-cartoon-set

# Extracting the data
filename = '/content/cartoon-faces-googles-cartoon-set.zip'
with zipfile.ZipFile(filename, 'r') as zip_ref:
    zip_ref.extractall('../content/')

# Creating the DataLoader after making the transformations.

# Toon data Transformations
data_transforms = transforms.Compose([transforms.Resize((128, 128)), # Note: Due to limited resources and time constraints, I chose a 64*G4 transformation but higher ones can be applied for better results.
                                      transforms.ToTensor(),
                                      transforms.Normalize(0.5, 0.5)])

# Toon Dataset and DataLoader
batch_size = 512 # Chose 512 because of computational limitations, however greater batch sizes have shown better results in my experience.
toon_dataset = datasets.ImageFolder(root='../content/cartoonset100k_jpg/',
                                  transform=data_transforms,
                                  target_transform=None) # Transforms for labels, not applicable here.
toon_dataloader = DataLoader(dataset=toon_dataset,
                          shuffle=True,
                          num_workers=os.cpu_count(), # Setting numworkers to what is available.
                          batch_size=batch_size)

"""### Visualizing the Google Toon Image Data"""

# Here we will take one batch(defined above) and visualize its forst 20 items.
d2l.set_figsize((4, 4))
for X, y in toon_dataloader:
  imgs = X[0:20, :, :, :].permute(0, 2, 3, 1)/2+0.5
  d2l.show_images(imgs, num_rows=4, num_cols=5)
  break

"""### The Generator

This generator will make up half of our GAN model. It will be responsible for creating the fake images that we want as the final output from this project. We will start by creating generator units(G_Unit), and use those to create the overall generator architecture(G_arch).
"""

class G_Unit(nn.Module):
    def __init__(self, in_channels, out_channels,  kernel_size=4, stride=2,
                 padding=1, **kwargs): # **kwargs implemented here because additional arguments(lr and betas) will be added during training.
        super(G_Unit, self).__init__(**kwargs)
        # Creating the generator unit. Using nn.Sequential for readability.
        self.unit = nn.Sequential(nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
                                nn.BatchNorm2d(out_channels), # Using batch normalization for better results.
                                nn.ReLU()
                                )

    def forward(self, X):
        output = self.unit(X)
        return output

G_arch = nn.Sequential( # Using nn.Sequential for readability.
    G_Unit(in_channels=100, out_channels=2048),      # The first layer takes a tensor of 100 channels(initialized during training) and outputs
    G_Unit(in_channels=1024*2, out_channels=1024*2), # 2048 channels(64*32 decreased from 64*64 because of computational limitations). The second to sixth layer
    G_Unit(in_channels=1024*2, out_channels=1024),   # mostly decrease the output channels gradually for optimal learning. These, along with the initial output
    G_Unit(in_channels=1024, out_channels=512),      # could be increased(more neurons) for better and more detail oriented learning.
    G_Unit(in_channels=512, out_channels=256),
    G_Unit(in_channels=256, out_channels=128),

    nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=1),
    nn.Tanh() # Tanh limits the range from -1 to 1, ideal for image related data.
    )

# Testing, remove after

# initalize model params
batch_size = 64

# input: 100 dimensional vector sampled from normal distrubtion
# we shape it with dummy dimensions for width and height
# becuase convtranspose2d layers operate on images
# (batchsize, 100, 1, 1)
dummy_input_Generator = torch.randn(batch_size, 100, 1, 1)
output_generator = G_arch(dummy_input_Generator)
# should be shape (batchsize, 3, 64, 64). generator generate
print(f"output_generator.shape: {output_generator.shape}")

# For testing purposes. Remove later.
class G_arch(nn.Module):
  def __init__(self, in_channels, out_channels,  kernel_size=4, stride=2,
                 padding=1, **kwargs):
    super(G_arch, self).__init__(**kwargs)
    self.gen = nn.Sequential(
        nn.ConvTranspose2d(in_channels=100, out_channels=2048, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(2048),
        nn.ReLU(),

        nn.ConvTranspose2d(in_channels=2048, out_channels=1024, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(1024),
        nn.ReLU(),

        nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(512),
        nn.ReLU(),

        nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(256),
        nn.ReLU(),

        nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(128),
        nn.ReLU(),

        nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.Tanh()
        )

  def forward(self, inp):
    return self.gen(inp)

"""### The Discriminator

The other half of our GAN model, the discriminator, takes the output from the generator, and tells us if it is real or fake. Feedback from the discriminator over time makes the generator better at creating fake images from random noise that look real. Here, like in the case of the generator, we will start by creating discriminator units(D_Unit), and use those to create the overall discriminator architecture(D_arch).
"""

class D_Unit(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2,
                 padding=1, alpha=0.2, **kwargs):
        super(D_Unit, self).__init__(**kwargs)
        self.unit = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
                                nn.BatchNorm2d(out_channels),
                                nn.LeakyReLU(alpha)
                                )

    def forward(self, X):
      output = self.unit(X)
      return output

D_arch = nn.Sequential(
    D_Unit(in_channels=3, out_channels=64*2),
    D_Unit(in_channels=64*2, out_channels=64*4),
    D_Unit(in_channels=64*4, out_channels=64*8),
    D_Unit(in_channels=64*8, out_channels=64*16),
    D_Unit(in_channels=64*16, out_channels=64*32),
    nn.Conv2d(64*32, 1, 4, 1, 0)
    )

# Testing, remove after.
batch_size = 64

# input: is our output from our generator
# which is a 3 channel image with shape (3, 64, 64)
# we could generate some dummy data, but lets use the generator output
# we could use this instead to generate dummy data dummy_input_Discriminator = torch.randn(batch_size,3,64,64)
output_discriminator = D_arch(output_generator)
# should be shape (batchsize, 1, 1, 1)
print(f"output_discriminator.shape: {output_discriminator.shape}")
# the last layer to obtain a single prediction value.
# real or fake

# For testing purposes. Remove later.
class D_arch(nn.Module):
  def __init__(self, in_channels=3, out_channels=1024, kernel_size=4, stride=2, padding=1, slope=0.2):
    super().__init__()
    self.disc = nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(128),
        nn.LeakyReLU(slope),

        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(256),
        nn.LeakyReLU(slope),

        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(512),
        nn.LeakyReLU(slope),

        nn.Conv2d(in_channels=512, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(out_channels),
        nn.LeakyReLU(slope),

        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(out_channels),
        nn.LeakyReLU(slope),

        nn.Conv2d(64 * 16, 1, 2, 1, 0)
        )

  def forward(self, X):
    return self.disc(X)

"""### Cartoon GAN Training

Here we will define a function to train our GAN model, the protocol outlined by AstonZhang, ZacharyC.Lipton, MuLi, and AlexanderJ.Smola in [Dive into Deep Learning](https://classic.d2l.ai/d2l-en-pytorch.pdf) was used as inspiration for our GAN training.
"""

def train(D_arch, G_arch, toon_dataloader, num_epochs, lr, # The usual parameters that we will need for training.
          initial_dim, device=d2l.try_gpu()):              # Here the initial_dim will be our initial input into the generator and device will ideally be set to gpu for efficient processing.

    # Moving the architectures to the preferred device.
    D_arch, G_arch = D_arch.to(device), G_arch.to(device)

    # Defining the keyword arguments for the models.
    trainer_hp = {'lr': lr, 'betas': [0.5, 0.999]}

    # Defining the loss function
    loss = nn.BCEWithLogitsLoss(reduction='sum')

    # Defining the optimizers
    trainer_D = torch.optim.Adam(D_arch.parameters(), **trainer_hp)
    trainer_G = torch.optim.Adam(G_arch.parameters(), **trainer_hp)

    # Initializing the parameters of both the discriminator and the generator using a normal distribution.
    for w in D_arch.parameters():
        nn.init.normal_(w, 0, 0.02)
    for w in G_arch.parameters():
        nn.init.normal_(w, 0, 0.02)


    # Creating the animation that will track the loss of the generator and the discriminator over time.
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
                            legend=['discriminator', 'generator'])
    animator.fig.subplots_adjust(hspace=0.3)

    # Making the training loop, going over a set number of epochs.
    for epoch in tqdm(range(1, num_epochs + 1)):
        # Setting the timer and metrics for d2l module usage.
        timer = d2l.Timer()
        metric = d2l.Accumulator(3)  # loss_D, loss_G, examples

        # Going over each batch one by one.
        for X, _ in toon_dataloader:
            batch_size = X.shape[0] # First dim of the batch is the batch size.
            Z = torch.normal(0, 1, size=(batch_size, initial_dim, 1, 1))
            X, Z = X.to(device), Z.to(device) # Moving image data to the same device the the models are in.
            # Updating loss_D, loss_G, examples
            metric.add(d2l.update_D(X, Z, D_arch, G_arch, loss, trainer_D),
                       d2l.update_G(Z, D_arch, G_arch, loss, trainer_G),
                       batch_size)

        # Making the initial generator input and normalizing it to showcase output examples.
        Z = torch.normal(0, 1, size=(21, initial_dim, 1, 1), device=device)

        # Creating the fake images and displaying them.
        fake_x = G_arch(Z).permute(0, 2, 3, 1) / 2 + 0.5
        imgs = torch.cat([
            torch.cat([fake_x[i * 7 + j].cpu().detach()
                       for j in range(7)], dim=1)
            for i in range(len(fake_x) // 7)], dim=0)
        animator.axes[1].cla()
        animator.axes[1].imshow(imgs)

        # Showing the generator and discriminator losses among other information.
        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]
        animator.add(epoch, (loss_D, loss_G))
    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '
          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')

# Train the network on GPU
# See how the loss of D and G change over time, and how the final image generations look like.
initial_dim, lr, num_epochs = 100, 0.005, 15
train(D_arch, G_arch, toon_dataloader, num_epochs, lr, initial_dim)

"""Todo list:
- Proofread and debug.
- Make the training code more readable.
- Add to the evaluative output.
- Try training with a higher number of epochs(100 maybe).
"""